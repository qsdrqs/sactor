[general]
max_translation_attempts = 6
max_verifier_harness_attempts = 6
timeout_seconds = 60 # timeout for the execution of generated code
system_message = '''
You are an expert in translating code from C to Rust. You will take all information from the user as reference, and will output the translated code into the format that the user wants.
'''
encoding = "o200k_base" # Encoding for the `tiktoken` library, default for GPT-4o model
model = "gpt-4o" # Default model to use

# LiteLLM Router Configuration - follows litellm config format
# Supports all providers that litellm supports: OpenAI, Anthropic, Google, Azure, AWS Bedrock,
# Cohere, DeepSeek, Groq, Ollama, vLLM, and 100+ other providers
[litellm]
[litellm.router_settings]
routing_strategy = "simple-shuffle"
num_retries = 2
timeout = 600

# Example: OpenAI configuration
[[litellm.model_list]]
model_name = "gpt-4o"
[litellm.model_list.litellm_params]
model = "openai/gpt-4o-2024-08-06"
api_key = "os.environ/OPENAI_API_KEY"
# rpm = 500  # Optional: requests per minute limit

# Example: Azure OpenAI configuration
[[litellm.model_list]]
model_name = "azure-gpt-4o"
[litellm.model_list.litellm_params]
model = "azure/your-deployment-name"
api_key = "os.environ/AZURE_API_KEY"
api_base = "os.environ/AZURE_API_BASE"
api_version = "2024-12-01-preview"
# rpm = 900  # Optional: requests per minute limit

# Example: Anthropic Claude configuration
[[litellm.model_list]]
model_name = "claude-3-5-sonnet-20241022"
[litellm.model_list.litellm_params]
model = "anthropic/claude-3-5-sonnet-20241022"
api_key = "os.environ/ANTHROPIC_API_KEY"

# Example: Google Gemini configuration
[[litellm.model_list]]
model_name = "gemini-2.0-flash"
[litellm.model_list.litellm_params]
model = "gemini/gemini-2.0-flash-exp"
api_key = "os.environ/GEMINI_API_KEY"
# rpm = 400  # Optional: requests per minute limit

# Example: Ollama configuration (local)
[[litellm.model_list]]
model_name = "llama3.3"
[litellm.model_list.litellm_params]
model = "ollama/llama3.3"
api_base = "http://localhost:11434"
# rpm = 100  # Optional: requests per minute limit

# Add more models following the same pattern
# For complete list of supported providers and formats, see: https://docs.litellm.ai/docs/providers

[test_generator]
max_attempts = 6
timeout_seconds = 60

[test_runner]
timeout_seconds = 60
