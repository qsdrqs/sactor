[general]
max_translation_attempts = 6
max_verifier_harness_attempts = 6
# If true, use c2rust translation results when the unidiomatic translator fails
unidiomatic_fallback_c2rust = false
unidiomatic_fallback_c2rust_fix_attempts = 6
timeout_seconds = 60 # timeout for the execution of generated code
command_output_byte_limit = 40000 # Max bytes captured from subprocess stdout/stderr before truncation
const_global_max_translation_len = 2048 # Max accepted length of baseline const global definitions
max_llm_input_tokens = 20480 # Maximum tokens allowed in a single LLM prompt before truncation
system_message = '''
You are an expert in translating code from C to Rust. You will take all information from the user as reference, and will output the translated code into the format that the user wants.
'''
encoding = "o200k_base" # Encoding for the `tiktoken` library, default for GPT-4o model
model = "gpt-4o" # Default model to use

[test_generator]
max_attempts = 6
timeout_seconds = 60

[test_runner]
timeout_seconds = 60

[verifier]

[verifier.selftest]
enabled = true
samples_path = ""
struct_spec_path = ""

[logging]
# Minimum level that appears on stdout (DEBUG, PROMPT, RESPONSE, INFO, WARNING, ERROR, CRITICAL)
console_level = "DEBUG"
# Minimum level written to timestamped log files (same set of names or numeric values)
file_level = "DEBUG"
# Emit ANSI colour codes when the console is a TTY
color = true
# Also write structured JSONL entries alongside the text log
jsonl = true
# Persist LLM prompts/responses to a separate transcript file when enabled
prompt_trace = false
# Filename pattern for text logs; `{timestamp}` expands using `timestamp_format`
filename_pattern = "sactor-{timestamp}.log"
# Timestamp format used in generated log filenames
timestamp_format = "%Y%m%dT%H%M%S"
# Subdirectory (under result_dir) where logs are stored by default
subdir = "logs"

# LiteLLM Router Configuration - follows litellm config format
# Supports all providers that litellm supports: OpenAI, Anthropic, Google, Azure, AWS Bedrock,
# Cohere, DeepSeek, Groq, Ollama, vLLM, and 100+ other providers
[litellm]
[litellm.router_settings]
routing_strategy = "simple-shuffle"
num_retries = 2
timeout = 600

# Example: OpenAI configuration
[[litellm.model_list]]
model_name = "gpt-4o"
[litellm.model_list.litellm_params]
model = "openai/gpt-4o-2024-08-06"
api_key = "os.environ/OPENAI_API_KEY"
# rpm = 500  # Optional: requests per minute limit

# Example: Azure OpenAI configuration
[[litellm.model_list]]
model_name = "azure-gpt-4o"
[litellm.model_list.litellm_params]
model = "azure/your-deployment-name"
api_key = "os.environ/AZURE_API_KEY"
api_base = "os.environ/AZURE_API_BASE"
api_version = "2024-12-01-preview"
# rpm = 900  # Optional: requests per minute limit

# Example: Anthropic Claude configuration
[[litellm.model_list]]
model_name = "claude-3-5-sonnet-20241022"
[litellm.model_list.litellm_params]
model = "anthropic/claude-3-5-sonnet-20241022"
api_key = "os.environ/ANTHROPIC_API_KEY"

# Example: Google Gemini configuration
[[litellm.model_list]]
model_name = "gemini-2.0-flash"
[litellm.model_list.litellm_params]
model = "gemini/gemini-2.0-flash-exp"
api_key = "os.environ/GEMINI_API_KEY"
# rpm = 400  # Optional: requests per minute limit

# Example: Ollama configuration (local)
[[litellm.model_list]]
model_name = "llama3.3"
[litellm.model_list.litellm_params]
model = "ollama/llama3.3"
api_base = "http://localhost:11434"
# rpm = 100  # Optional: requests per minute limit

# Add more models following the same pattern
# For complete list of supported providers and formats, see: https://docs.litellm.ai/docs/providers
